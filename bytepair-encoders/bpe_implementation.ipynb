{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytearray'>\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text,\"utf-8\")\n",
    "print(bytearray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  BPE tokenizers have a vocabulary where we have a token ID for whole words or subwords instead of each character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Algorithm outline\n",
    "### 1. Identify frequent pairs\n",
    "\n",
    "In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters)\n",
    "\n",
    "### 2. Replace and record\n",
    "\n",
    "Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256)\n",
    "Record this mapping in a lookup table\n",
    "The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's 50,257)\n",
    "\n",
    "### 3. Repeat until no gains\n",
    "\n",
    "Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    "Stop when no further compression is possible (e.g., no pair occurs more than once)\n",
    "Decompression (decoding)\n",
    "\n",
    "To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.inverse_vocab = {}\n",
    "        self.bpe_merges = {}\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i!=0:\n",
    "                processed_text.append(\"G\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        \n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend_char for char in sorted(set(processed_text)) if char not in unique_chars]\n",
    "        if \"G\" not in unique_chars:\n",
    "            unique_chars.append(\"G\")\n",
    "        \n",
    "        self.vocab = {i:char for i,char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char:i for i,char in self.vocab.items()}\n",
    "\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    idx = len(self.vocab)\n",
    "                    self.vocab[idx] = token\n",
    "                    self.inverse_vocab[token] = idx\n",
    "            \n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode = \"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "        \n",
    "        for (p0,p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
